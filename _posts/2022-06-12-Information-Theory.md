---
keywords: fastai
description: Inductive inference, is the problem of reasoning under conditions of incomplete information, or `uncertainty`. According to Shannon's theory, information and uncertainty are two sides of the same coin: the more uncertainty there is, the more information we gain by removing the uncertainty. 
title: Information 
nb_path: _notebooks/2022-06-12-Information-Theory.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-06-12-Information-Theory.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Logarithm-in-Information-Theory">Logarithm in Information Theory<a class="anchor-link" href="#Logarithm-in-Information-Theory"> </a></h3><p>It is important to know binary logarithm $log_2$ to understand mathematical calculation related to Information Throry.</p>
<p>Logarithm is the another way to represent indices or power</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$log_x(y) = z \ represent \ x^z=y \\
\therefore log_2(4)=2 \ as \  2^2 = 4 \ and \ log_2(8)=3 \ as \ 2^3 = 8 $$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The three basic laws of logarithms:
$$
log A + logB= logAB \\
logA^n = nlogA \\
logA-logB = log \frac{A}{B}$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Entropy">Entropy<a class="anchor-link" href="#Entropy"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Entropy is a measure of the uncertainty (randomeness) of a probability distribution.</p>
<p>Mathematicaly is it defined as `The entropy of a random rariable $X$ with a probability mas function p(x) is defined by</p>
<p>Shannon <code>entropy</code> formula:</p>
<p>{% raw %}
$$H(X) = -\sum_{i=1}^n p_i \log_2 p_i$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Shannon-found-that-entropy-was-the-only-funcion-satisfying-the-three-requirements">Shannon found that entropy was the only funcion satisfying the three requirements<a class="anchor-link" href="#Shannon-found-that-entropy-was-the-only-funcion-satisfying-the-three-requirements"> </a></h3><ol>
<li>$H(X)$ is always non-negtive, since information cann't be lost.</li>
<li>The uniform distribution maximize $H(X)$, since it also maximizes uncertainty.</li>
<li>The additivity property which relate the sum of entropies of two independent events. For instance, in thermodynamics, the total entropy of two isolated system which co-exist in equilibrium is the same as the sum of the entropies each system in isolation.m </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Problems:">Problems:<a class="anchor-link" href="#Problems:"> </a></h3><ol>
<li>For an event which is certain to happen, what is the entropy of the event?</li>
<li>For an event which is uncertain, what is the entropy of the event?</li>
<li>For n equiprobable event, what is the entropy of the event?</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With respect to the notion of <code>surprise</code> in the context of information theory:</p>
<ol>
<li>Define what is actually mean by being surprised.</li>
<li>Describe how it is related to the likelihood of an event happening.</li>
<li>True or False: The less likely the occurrence of an event, the smaller information it conveys.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>The notion of surprise is directly related to the likelihood of an event happenning. Mathematically it is inversely propotional to the probability of that event. A high-probability event gives less information than los-probability event. </li>
<li>Learning that a high-probability event has taken place, for instance the sun rising, is much less of a surprise than a low-probability event, for instance the moon landing.</li>
<li>Therefore, the less likely the likely  the occurence of an event, the greater information it conveys. </li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Reference: <a href="https://arxiv.org/ftp/arxiv/papers/2201/2201.00650.pdf">https://arxiv.org/ftp/arxiv/papers/2201/2201.00650.pdf</a></p>

</div>
</div>
</div>
</div>
 

