{
  
    
        "post0": {
            "title": "Object Detection",
            "content": "Object detection is a computer vision task that involves two tasks: . Localizing one or more objects within an image, and | | Classifying each object in the image | | . YOLO . YOLO is a family of object detection networks that improved over the years throught the following version; YOLOv1, YOLOv2, YOLOv3 and YOLOv4. The YOLO family of models is a series of end-to-end deep learning models designed for fast object detection, developed by Joseph Redmon, etal. Thought is is no longer the most accurate object detection algorithms, it is a very good choice when you need real-time detection, withoug loss of too much accuracy. . How it works . YOLO first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processsing is used to refine the bounding boxes, eliminate duplicate detctions and rescore the bxes based on other objects in the scene. . Divide the image into number of SxS cells | IF Center of an object falls into a grid cess, that grid cell is responsible for detecting that object. | Each grid cell predict B bounding boxes and conficence score for those boxes. | If no object exists in that cess, the conficence score should be zero othersise confidence score is equal to Pr(Object)IoU(truth|pred) | Each bounding box consists of 5 prediction: x,y,w,h and confidnece. The (x,y) coordinates represent the center of the box relative to the bounds of grid cell. The width and height are predicted relative to the whole image. Finally the conficence preeiction represents the IoU between the predicted box and ground truth box. | Each grid cess also predict C conditional class probabilities, Pr(class|Object). YOLO predict only one set of class probabilities per grid cess, regredless of the number of boxes B. At test time it multiply the conditional class probabilityies and the indivicual box conficence prediction. . For example: To evluate Pascal VOC, YOLO Paper used S = 7, B = 2, and Pascal VOC has 20 label so C=20 . The image will be divided into 7x7 cell | For B = 2, each grid cell will predict 2 bounding box and confidence score for each. | The bounding box contain (x, y, w, h and conficence) for each grid cell. So the output vector so far is 7x7x(2x5) | Each grid cess also predict 1 Conditional class probabilities for each class. So the output vector will be 7x7x(2x5+C) But C = 20 for Pascal VOC so the final output vector will be 7X7X(2X5+20) = 7x7x30 | Architecture .",
            "url": "https://skj092.github.io/blog/2022/06/16/Object-Detection.html",
            "relUrl": "/2022/06/16/Object-Detection.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Blur the face in an image or video",
            "content": "Blurring the face area of people from videos is done in all news channels and to hide the identity of a person. With computer vision, We can automatically detect the face region of the person and use it to blur the image. In this project we will build a computer vision model which can detect face in an image and blur it. . 1. Read an image 2. Convert it to grayscale 3. load Cascade Classifier 4. Detect Faces 5. Draw Bounding Box 6. Blur the face . import cv2 import matplotlib.pyplot as plt . Step-1: Read an image . img = cv2.imread(&#39;../images/face.jpg&#39;) plt.imshow(img[:,:,::-1]) # opencv read an image into BGR mode, to convert it into RGB we reverse the image array . &lt;matplotlib.image.AxesImage at 0x1e1cedff610&gt; . Step-2: Convert the image to grayscale . gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) plt.imshow(gray, cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x29d1fcbd9f0&gt; . Step-3: Loading OpenCV CascadeClassifier . face_classifier = cv2.CascadeClassifier(&#39;haarcascades/haarcascade_frontalface_default.xml&#39;) . Step-4: Detecting faces in the grayscaled image . faces = face_classifier.detectMultiScale(gray) . Step-5: Draw Blounding Box around the detected faces . for (x,y,w,h) in faces: cv2.rectangle(img, (x,y), (x+w,y+h), (127,0,255),2) plt.imshow(img[:,:,::-1]) . Step-6: Blur the face . ROI = img[y:y+h, x:x+w] # blur the face region blur_face = cv2.GaussianBlur(ROI, (91,91),0) . img[y:y+h, x:x+w] = blur_face . plt.imshow(img[:,:,::-1]) . &lt;matplotlib.image.AxesImage at 0x29d25136620&gt; .",
            "url": "https://skj092.github.io/blog/2022/05/24/Detect-Face-And-Blur.html",
            "relUrl": "/2022/05/24/Detect-Face-And-Blur.html",
            "date": " • May 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Random Thoughts",
            "content": "Random Thoughts . Writing is one of the best habits one can have. I have tried again and again to make it my daily habit. Though I used to write in a freeform style whatever comes into my mind. It calms the mind down and I feel relaxed. But I thought writing publically is not my cup of tea. However, this article changed my thinking. . You can write freestyle publically too. It feels more natural and easily readable than other styles. Depends on your goal on what you want to achieve in your writing. Sometimes people want to be highlighted by writing but I have a different goal, I want to improve myself by writing and I don’t have any aim to be highlighted. . Trying to get highlighted have a downside, we always focus on what people want to see and we do or try to show them off. Instead, we should focus on improving our skills and doing what we like. Being happy, relaxed, and calm improve our learning and overall mental health. While chasing a long-term goal you need to be in your natural style - calm, relaxed, and happy. . I am also chasing my long-term goal. Even though I am practicing machine learning for the past 2 years, I still feel like a beginner. But it happens with everyone, in the journey of gaining the knowledge you spend more time in the low confident zone. I must be one the middle of the Dunning-Kruger Curve. . . While you feel uncomfortable in the low confident zone, it is also the most beautiful and important zone. It teaches us lots of valuable things in life. It teaches us - Discipline and Persistence which is the amazing tool you need to achieve anything in life. . On the journey of being a lifelong learner sometimes I think, “Do I have enough speed ?”. However, speed doesn’t matter as long as you keep showing up every day. Showing up every day is the hardest part, you keep getting off the track again and again but you also need to learn how to bounce back. It is similar to practicing meditation. You keep getting distracted but the important thing is to get back on track. . Your environment is one of the biggest factors in bouncing back. Our environment has the biggest impact on us. Therefore we must be aware of our environment. Where and with whom do we spend our time with? How is it helpful to achieve what you want? You may not have the kind of environment want but you can always create your own environment. In the world of the internet, it is all possible. . Set your own environment, keep only those people who are helpful to you, Keep bouncing back, Relax and enjoy this journey. .",
            "url": "https://skj092.github.io/blog/writing/2022/02/15/Random-Thoughts.html",
            "relUrl": "/writing/2022/02/15/Random-Thoughts.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Evolution of Artificial Neural Network(ANN)",
            "content": "Artificial Intelligence . Artificial intelligence (AI) is a branch of computer science capable of performing tasks that typically require human intelligence. . . What is Deep Learning? . deep learning is a more approachable name for an artificial neural network. The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow. . Machine learning is the science of getting computers to act without being explicitly programmed. | . ANN:Artificial Neural Networks . ANNs are inspired by biological neurons found in cerebral cortex of our brain. | . . A neuron or nerve cell is an electrically excitable cell that communicates with other cells via specialized connections called synapses. | ANNs are core of deep learning. Hence one of the most important topic to understand. | . Perceptron . A Perceptron is one of the simplese ANN architectures, invented in 1957 by Frank Rosenblatt. . A perceptron works similar to a biological neuron. A biological neuron receives electrical signals from its dendriles, modulates the electrical signals in various amounts, and then fires an output signal through its synapses only when the total strength of the input signals exceeds a certain threshold. The output is then fed to another neuron, and so forth. . To model the biological phenomenon, the artificial neuron performs two consecutive functions:it calculates the weighted sum of the inputs to represent the totgal strength of the input signals, and it applies a step function to the result to determine whether to fire the ourput 1 if the signal exceeds a certain threshold of 0 if the signal doesn&#39;t exceed the threshold. . How Peceptron Works? . The perceptron&#39;s learning logic goes like this: . The neuron calculate the weighted sum and applies the activation function to make a prediction y^. This is called the feedforward process: | $$y hat{} = activation left( sum nolimits x_i cdot w_i + b right)$$ . It compares the output prediction with the correct label to calculate the error: | $$error = y-y hat{}$$ . It then update the weight. If the prediction is too high, it adjust the weight to make a lower prediction the next time, and vice versa. . | Repeat! . | Here&#39;s how a since perceptron works to classify two classes : . . Drawback of perceptron. Sometimes its not possible to get desired result with only perceptron. In the below example you can see the model in not able to draw a line to classify the data(linearly inseperable data) . . Indroduction of ANN If you increase the number of neuron then you cann see model works pretty well. The stack of more than one neuron is called Multi Layer Perceptron or ANN. . . ANN using Keras . import tensorflow as tf # Helper libraries import numpy as np from tensorflow.keras import initializers from tensorflow.python.keras import activations print(tf.__version__) # downloading fashion_mnist data fashion_mnist = tf.keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;] train_images = train_images / 255.0 test_images = test_images / 255.0 activation = tf.keras.activations.relu model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=activation), tf.keras.layers.Dense(10) ]) model.compile(optimizer=&#39;adam&#39;,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=[&#39;accuracy&#39;]) # model summary model.summary() . c: users sonu.ramkumar.jha desktop experiments env lib site-packages numpy _distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs: c: users sonu.ramkumar.jha desktop experiments env lib site-packages numpy .libs libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll c: users sonu.ramkumar.jha desktop experiments env lib site-packages numpy .libs libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll warnings.warn(&#34;loaded more than 1 DLL from .libs:&#34; . 2.5.0 Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 10) 1290 ================================================================= Total params: 101,770 Trainable params: 101,770 Non-trainable params: 0 _________________________________________________________________ . model.fit(train_images, train_labels, epochs=10) test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(&#39;test_loss&#39;, test_loss) print(&#39;test_accuracy&#39;, test_acc) . Epoch 1/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5000 - accuracy: 0.8255 Epoch 2/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3751 - accuracy: 0.8645 Epoch 3/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3388 - accuracy: 0.8773 Epoch 4/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3142 - accuracy: 0.8843 Epoch 5/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2957 - accuracy: 0.8922 Epoch 6/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2797 - accuracy: 0.8971 Epoch 7/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2685 - accuracy: 0.8991 Epoch 8/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2577 - accuracy: 0.9028 Epoch 9/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2476 - accuracy: 0.9076 Epoch 10/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2388 - accuracy: 0.9105 313/313 - 0s - loss: 0.3403 - accuracy: 0.8798 test_loss 0.34025073051452637 test_accuracy 0.879800021648407 .",
            "url": "https://skj092.github.io/blog/2020/07/05/Evolution-of-Artificial-Neural-Network(ANN).html",
            "relUrl": "/2020/07/05/Evolution-of-Artificial-Neural-Network(ANN).html",
            "date": " • Jul 5, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pytorch - Your Favourite Deep Learning Library",
            "content": "Some most common operation using Pytorch . Pytorch is an open source machine learning library probably easiest to start with. It is built by Facebook. In this notebook I will write and discuss five most common and important functions in pytorch. . torch.transpose() | torch.stack() | torch.where() | torch.backward() | torch.add() | . 1. transpose() . In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing. Here we will transpose tensor using pytorch transpose function. . . x is a 2-D tensor. we swapped dimension 0 to dimension 1 using torch.transpose() . . we swapped dimension 1 to dimension 0. . . We need to put the dimension which is going to be swapped without. And if we put same dimension twice it has no effect. . Transpose function is very useful while matrix multiplication . 2. stack() . This function conconate every new tensor in new dimension. It add every new tensor to additional dimension . . Here we stacked b over a and created an additional dimesion. . . Here b is the stack of 3 a tensor . To make an stack, all tensors need to be in equal size. . While working with images data, stack function is useful for numerical calculation. . For example- to calculate statistical values like mean, median, etc. Stacking makes easy to calculate all of these. . 3. where() . Returns tensor of element with given condition . . torch.where() returns a tensor with values y if x&gt;0 else it return x. . . here we find the difference between x and y. This function can be used as a loss function in binary classification problems, where the target variable is similar like x and predicted value like y. . . If size of y is less then x then pytorch automatically expand y to the size of x without additional memory. However if the size of y is greater then x, the it doesn’t work. . This function can be used as a loss function in binary classification problems. . 4. backward() . This function is most important function in mathine leanring. It calculate the gradient of a function. . . To calculate gradient we need to set requires_grad = True. . While updating the weight in gradient descent backword function is used to update the weight. . 5. add() . To add two tensor . . Here add function just work like + operator. . . As you can see above the size of y is not the size of x. But pytorch expand the size of y by replacating its data and do elementwise operation. . . Here the size of y is greator then the size of x but x doesn’t expand. Pytorch expand in every dimension, if it increase the dimension of x then it will become 3*3 still doesn’t match the size of y. . It is very helpful while doing tensor operation. Additional operator is one of the most used operator . Conclusion . We have covered five most useful pytorch function. Go the the pytorch documentation and write five function of your choice. . Reference . Official Pytorch Documentation | .",
            "url": "https://skj092.github.io/blog/markdown/2020/01/14/Pytorch.html",
            "relUrl": "/markdown/2020/01/14/Pytorch.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a mathematics graduate passionate about data science in general deep learning in particular. I love to learn new things and grow. I’ve spent the last 3 years growing as a developer through experience and education. . I prefer to think of myself as an open-minded, down to earch and outgoing person. I care very much for people who I come across in my life and try to make their lives easier. I love nature as well, this is why I like to go on camping, hiking and all sorts of adventure activities. . I also like to read about anthropology, psychology and history to know how we all grow and why we are what we are. I am not sure if it’s true but I like to think that whatever we do it has a reason most of the time we are not aware of our action and thoughts but it always has the reason. . I am currently working as a Web Developer at Accenture Solution and solve machine learning problems in my free time. . Projects : . Pets Breeds Classification | Fruit Image Object Detection | Detect and Blur Face | Leaf Disease Classification | Poetry Generator | House Price Prediction | Credit Risk Analysis | Review Scrapper | .",
          "url": "https://skj092.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://skj092.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}