{
  
    
        "post0": {
            "title": "A New Beginning",
            "content": "A New Beginning . Writing is one of the best habits one can have. I have tried again and again to make it my daily habit. Though I used to write in a freeform style whatever comes into my mind. It calms the mind down and I feel relaxed. But I thought writing publically is not my cup of tea. However, this article changed my thinking. . You can write freestyle on publically too. It feels more natural and easy readable than other styles. Depends on your goal on what you want to achieve in my writing. Sometimes people want to be highlighted by writing but I have a different goal, I want to improve myself by writing and I don’t have any aim to be highlighted. . Trying to get highlighted have a downside, we always focus on what people want to see and we do or try to show them off. Instead, we should focus on improving our skills and doing what we like. Being happy, relaxed and calm improve our learning and overall mental health. While chasing a long term goal you need to be in your natural style - calm, relaxed, and happy. . I am also chasing my long term goal. Even though I am practising machine learning for the past 2 years, I still feel like a beginner. But it happens with everyone, in the journey of gaining knowledge you spend more time in the low confident zone. I must be one the middle of the Dunning-Kruger Curve. . . While you feel uncomfortable in the low confident zone, it is also the most beautiful and important zone. It teaches us lots of valuable things in life. It teaches us - Discipline and Persistence which is the amazing tool you need to achieve anything in life. . On the journey of being a lifelong learner sometimes I think, “Do I have enough speed ?”. However, speed doesn’t matter as long as you keep showing up every day. Showing up every day is the hardest part, you keep getting off the track again and again but you also need to learn how to bounce back. It is similar to practising meditation. You keep getting distracted but the important thing is to get back on track. . Your environment is one of the biggest factors in bouncing back. Our environment has the biggest impact on us. Therefore we must be aware of our environment. Where and with whom do we spend our time with? How is it helpful to achieve what you want? You may not have the kind of environment want but you can always create your own environment. In the world of the internet, it is all possible. . Set your own environment, keep only those people who is helpful for you, Keep bouncing back, Relax and enjoy this journey. .",
            "url": "https://skj092.github.io/blog/writing/2022/02/15/A-New-Beginning.html",
            "relUrl": "/writing/2022/02/15/A-New-Beginning.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Evolution of Artificial Neural Network(ANN)",
            "content": "Artificial Intelligence . Artificial intelligence (AI) is a branch of computer science capable of performing tasks that typically require human intelligence. . . What is Deep Learning? . deep learning is a more approachable name for an artificial neural network. The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow. . Machine learning is the science of getting computers to act without being explicitly programmed. | . ANN:Artificial Neural Networks . ANNs are inspired by biological neurons found in cerebral cortex of our brain. | . . A neuron or nerve cell is an electrically excitable cell that communicates with other cells via specialized connections called synapses. | ANNs are core of deep learning. Hence one of the most important topic to understand. | . Perceptron . A Perceptron is one of the simplese ANN architectures, invented in 1957 by Frank Rosenblatt. . A perceptron works similar to a biological neuron. A biological neuron receives electrical signals from its dendriles, modulates the electrical signals in various amounts, and then fires an output signal through its synapses only when the total strength of the input signals exceeds a certain threshold. The output is then fed to another neuron, and so forth. . To model the biological phenomenon, the artificial neuron performs two consecutive functions:it calculates the weighted sum of the inputs to represent the totgal strength of the input signals, and it applies a step function to the result to determine whether to fire the ourput 1 if the signal exceeds a certain threshold of 0 if the signal doesn&#39;t exceed the threshold. . How Peceptron Works? . The perceptron&#39;s learning logic goes like this: . The neuron calculate the weighted sum and applies the activation function to make a prediction y^. This is called the feedforward process: | $$y hat{} = activation left( sum nolimits x_i cdot w_i + b right)$$ . It compares the output prediction with the correct label to calculate the error: | $$error = y-y hat{}$$ . It then update the weight. If the prediction is too high, it adjust the weight to make a lower prediction the next time, and vice versa. . | Repeat! . | Here&#39;s how a since perceptron works to classify two classes : . . Drawback of perceptron. Sometimes its not possible to get desired result with only perceptron. In the below example you can see the model in not able to draw a line to classify the data(linearly inseperable data) . . Indroduction of ANN If you increase the number of neuron then you cann see model works pretty well. The stack of more than one neuron is called Multi Layer Perceptron or ANN. . . ANN using Keras . import tensorflow as tf # Helper libraries import numpy as np from tensorflow.keras import initializers from tensorflow.python.keras import activations print(tf.__version__) # downloading fashion_mnist data fashion_mnist = tf.keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;] train_images = train_images / 255.0 test_images = test_images / 255.0 activation = tf.keras.activations.relu model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=activation), tf.keras.layers.Dense(10) ]) model.compile(optimizer=&#39;adam&#39;,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=[&#39;accuracy&#39;]) # model summary model.summary() . c: users sonu.ramkumar.jha desktop experiments env lib site-packages numpy _distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs: c: users sonu.ramkumar.jha desktop experiments env lib site-packages numpy .libs libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll c: users sonu.ramkumar.jha desktop experiments env lib site-packages numpy .libs libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll warnings.warn(&#34;loaded more than 1 DLL from .libs:&#34; . 2.5.0 Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 10) 1290 ================================================================= Total params: 101,770 Trainable params: 101,770 Non-trainable params: 0 _________________________________________________________________ . model.fit(train_images, train_labels, epochs=10) test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print(&#39;test_loss&#39;, test_loss) print(&#39;test_accuracy&#39;, test_acc) . Epoch 1/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5000 - accuracy: 0.8255 Epoch 2/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.3751 - accuracy: 0.8645 Epoch 3/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3388 - accuracy: 0.8773 Epoch 4/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.3142 - accuracy: 0.8843 Epoch 5/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2957 - accuracy: 0.8922 Epoch 6/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2797 - accuracy: 0.8971 Epoch 7/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.2685 - accuracy: 0.8991 Epoch 8/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2577 - accuracy: 0.9028 Epoch 9/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2476 - accuracy: 0.9076 Epoch 10/10 1875/1875 [==============================] - 3s 1ms/step - loss: 0.2388 - accuracy: 0.9105 313/313 - 0s - loss: 0.3403 - accuracy: 0.8798 test_loss 0.34025073051452637 test_accuracy 0.879800021648407 .",
            "url": "https://skj092.github.io/blog/2020/07/05/Evolution-of-Artificial-Neural-Network(ANN).html",
            "relUrl": "/2020/07/05/Evolution-of-Artificial-Neural-Network(ANN).html",
            "date": " • Jul 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Pytorch - Your Favourite Deep Learning Library",
            "content": "Some most common operation using Pytorch . Pytorch is an open source machine learning library probably easiest to start with. It is built by Facebook. In this notebook I will write and discuss five most common and important functions in pytorch. . torch.transpose() | torch.stack() | torch.where() | torch.backward() | torch.add() | . 1. transpose() . In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing. Here we will transpose tensor using pytorch transpose function. . . x is a 2-D tensor. we swapped dimension 0 to dimension 1 using torch.transpose() . . we swapped dimension 1 to dimension 0. . . We need to put the dimension which is going to be swapped without. And if we put same dimension twice it has no effect. . Transpose function is very useful while matrix multiplication . 2. stack() . This function conconate every new tensor in new dimension. It add every new tensor to additional dimension . . Here we stacked b over a and created an additional dimesion. . . Here b is the stack of 3 a tensor . To make an stack, all tensors need to be in equal size. . While working with images data, stack function is useful for numerical calculation. . For example- to calculate statistical values like mean, median, etc. Stacking makes easy to calculate all of these. . 3. where() . Returns tensor of element with given condition . . torch.where() returns a tensor with values y if x&gt;0 else it return x. . . here we find the difference between x and y. This function can be used as a loss function in binary classification problems, where the target variable is similar like x and predicted value like y. . . If size of y is less then x then pytorch automatically expand y to the size of x without additional memory. However if the size of y is greater then x, the it doesn’t work. . This function can be used as a loss function in binary classification problems. . 4. backward() . This function is most important function in mathine leanring. It calculate the gradient of a function. . . To calculate gradient we need to set requires_grad = True. . While updating the weight in gradient descent backword function is used to update the weight. . 5. add() . To add two tensor . . Here add function just work like + operator. . . As you can see above the size of y is not the size of x. But pytorch expand the size of y by replacating its data and do elementwise operation. . . Here the size of y is greator then the size of x but x doesn’t expand. Pytorch expand in every dimension, if it increase the dimension of x then it will become 3*3 still doesn’t match the size of y. . It is very helpful while doing tensor operation. Additional operator is one of the most used operator . Conclusion . We have covered five most useful pytorch function. Go the the pytorch documentation and write five function of your choice. . Reference . Official Pytorch Documentation | .",
            "url": "https://skj092.github.io/blog/markdown/2020/01/14/Pytorch.html",
            "relUrl": "/markdown/2020/01/14/Pytorch.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a mathematics graduate passionate about data science in general deep learning in particular. . Projects : . Data Analysis | Machine Learning | . Skills and Certificates : . Data Science, Imarticus Learning | Python | Deep Learning with PyTorch Zero to GANs | . Education : . B.Sc Mathematics, Mithibai College, Mumbai University 04/2015-03/2018 Main Courses Algebra, Calcular, Descrete Mathematics, Numerical Analysis . Courses and Workshops . Machine Learning A-Z | Practical Deep Learning for Coder | Mini MTTS 2016 : Mathematics Research Training Workshop | .",
          "url": "https://skj092.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://skj092.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}